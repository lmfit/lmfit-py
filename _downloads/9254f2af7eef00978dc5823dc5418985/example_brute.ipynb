{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nGlobal minimization using the ``brute`` method (a.k.a. grid search)\n===================================================================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook shows a simple example of using ``lmfit.minimize.brute`` that\nuses the method with the same name from ``scipy.optimize``.\n\nThe method computes the function\u2019s value at each point of a multidimensional\ngrid of points, to find the global minimum of the function. It behaves\nidentically to ``scipy.optimize.brute`` in case finite bounds are given on\nall varying parameters, but will also deal with non-bounded parameters\n(see below).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import copy\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom lmfit import Minimizer, Parameters, fit_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's start with the example given in the documentation of SciPy:\n\n\"We illustrate the use of brute to seek the global minimum of a function of\ntwo variables that is given as the sum of a positive-definite quadratic and\ntwo deep \u201cGaussian-shaped\u201d craters. Specifically, define the objective\nfunction f as the sum of three other functions, ``f = f1 + f2 + f3``. We\nsuppose each of these has a signature ``(z, *params), where z = (x, y)``,\nand params and the functions are as defined below.\"\n\nFirst, we create a set of Parameters where all variables except ``x`` and\n``y`` are given fixed values.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "params = Parameters()\nparams.add_many(\n        ('a', 2, False),\n        ('b', 3, False),\n        ('c', 7, False),\n        ('d', 8, False),\n        ('e', 9, False),\n        ('f', 10, False),\n        ('g', 44, False),\n        ('h', -1, False),\n        ('i', 2, False),\n        ('j', 26, False),\n        ('k', 1, False),\n        ('l', -2, False),\n        ('scale', 0.5, False),\n        ('x', 0.0, True),\n        ('y', 0.0, True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Second, create the three functions and the objective function:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def f1(p):\n    par = p.valuesdict()\n    return (par['a'] * par['x']**2 + par['b'] * par['x'] * par['y'] +\n            par['c'] * par['y']**2 + par['d']*par['x'] + par['e']*par['y'] + par['f'])\n\n\ndef f2(p):\n    par = p.valuesdict()\n    return (-1.0*par['g']*np.exp(-((par['x']-par['h'])**2 +\n            (par['y']-par['i'])**2) / par['scale']))\n\n\ndef f3(p):\n    par = p.valuesdict()\n    return (-1.0*par['j']*np.exp(-((par['x']-par['k'])**2 +\n            (par['y']-par['l'])**2) / par['scale']))\n\n\ndef f(params):\n    return f1(params) + f2(params) + f3(params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Just as in the documentation we will do a grid search between ``-4`` and\n``4`` and use a stepsize of ``0.25``. The bounds can be set as usual with\nthe ``min`` and ``max`` attributes, and the stepsize is set using\n``brute_step``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "params['x'].set(min=-4, max=4, brute_step=0.25)\nparams['y'].set(min=-4, max=4, brute_step=0.25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Performing the actual grid search is done with:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fitter = Minimizer(f, params)\nresult = fitter.minimize(method='brute')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ", which will increment ``x`` and ``y`` between ``-4`` in increments of\n``0.25`` until ``4`` (not inclusive).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "grid_x, grid_y = [np.unique(par.ravel()) for par in result.brute_grid]\nprint(grid_x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The objective function is evaluated on this grid, and the raw output from\n``scipy.optimize.brute`` is stored in the MinimizerResult as\n``brute_<parname>`` attributes. These attributes are:\n\n``result.brute_x0`` -- A 1-D array containing the coordinates of a point at\nwhich the objective function had its minimum value.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(result.brute_x0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``result.brute_fval`` -- Function value at the point x0.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(result.brute_fval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``result.brute_grid`` -- Representation of the evaluation grid. It has the\nsame length as x0.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(result.brute_grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``result.brute_Jout`` -- Function values at each point of the evaluation\ngrid, i.e., Jout = func(\\*grid).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(result.brute_Jout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Reassuringly, the obtained results are identical to using the method in\nSciPy directly!**\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Example 2: fit of a decaying sine wave\n\nIn this example, we will explain some of the options of the algorithm.\n\nWe start off by generating some synthetic data with noise for a decaying\nsine wave, define an objective function and create a Parameter set.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = np.linspace(0, 15, 301)\nnp.random.seed(7)\nnoise = np.random.normal(size=x.size, scale=0.2)\ndata = (5. * np.sin(2*x - 0.1) * np.exp(-x*x*0.025) + noise)\nplt.plot(x, data, 'b')\n\n\ndef fcn2min(params, x, data):\n    \"\"\"Model decaying sine wave, subtract data.\"\"\"\n    amp = params['amp']\n    shift = params['shift']\n    omega = params['omega']\n    decay = params['decay']\n    model = amp * np.sin(x*omega + shift) * np.exp(-x*x*decay)\n    return model - data\n\n\n# create a set of Parameters\nparams = Parameters()\nparams.add('amp', value=7, min=2.5)\nparams.add('decay', value=0.05)\nparams.add('shift', value=0.0, min=-np.pi/2., max=np.pi/2)\nparams.add('omega', value=3, max=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In contrast to the implementation in SciPy (as shown in the first example),\nvarying parameters do not need to have finite bounds in lmfit. However, in\nthat case they **do** need the ``brute_step`` attribute specified, so let's\ndo that:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "params['amp'].set(brute_step=0.25)\nparams['decay'].set(brute_step=0.005)\nparams['omega'].set(brute_step=0.25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our initial parameter set is now defined as shown below and this will\ndetermine how the grid is set-up.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "params.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we initialize a Minimizer and perform the grid search:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fitter = Minimizer(fcn2min, params, fcn_args=(x, data))\nresult_brute = fitter.minimize(method='brute', Ns=25, keep=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We used two new parameters here: ``Ns`` and ``keep``. The parameter ``Ns``\ndetermines the \\'number of grid points along the axes\\' similarly to its usage\nin SciPy. Together with ``brute_step``, ``min`` and ``max`` for a Parameter\nit will dictate how the grid is set-up:\n\n**(1)** finite bounds are specified (\"SciPy implementation\"): uses\n``brute_step`` if present (in the example above) or uses ``Ns`` to generate\nthe grid. The latter scenario that interpolates ``Ns`` points from ``min``\nto ``max`` (inclusive), is here shown for the parameter ``shift``:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "par_name = 'shift'\nindx_shift = result_brute.var_names.index(par_name)\ngrid_shift = np.unique(result_brute.brute_grid[indx_shift].ravel())\nprint(\"parameter = {}\\nnumber of steps = {}\\ngrid = {}\".format(par_name,\n      len(grid_shift), grid_shift))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If finite bounds are not set for a certain parameter then the user **must**\nspecify ``brute_step`` - three more scenarios are considered here:\n\n**(2)** lower bound (min) and brute_step are specified:\nrange = (min, min + Ns * brute_step, brute_step)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "par_name = 'amp'\nindx_shift = result_brute.var_names.index(par_name)\ngrid_shift = np.unique(result_brute.brute_grid[indx_shift].ravel())\nprint(\"parameter = {}\\nnumber of steps = {}\\ngrid = {}\".format(par_name, len(grid_shift), grid_shift))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**(3)** upper bound (max) and brute_step are specified:\nrange = (max - Ns * brute_step, max, brute_step)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "par_name = 'omega'\nindx_shift = result_brute.var_names.index(par_name)\ngrid_shift = np.unique(result_brute.brute_grid[indx_shift].ravel())\nprint(\"parameter = {}\\nnumber of steps = {}\\ngrid = {}\".format(par_name, len(grid_shift), grid_shift))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**(4)** numerical value (value) and brute_step are specified:\nrange = (value - (Ns//2) * brute_step, value + (Ns//2) * brute_step, brute_step)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "par_name = 'decay'\nindx_shift = result_brute.var_names.index(par_name)\ngrid_shift = np.unique(result_brute.brute_grid[indx_shift].ravel())\nprint(\"parameter = {}\\nnumber of steps = {}\\ngrid = {}\".format(par_name, len(grid_shift), grid_shift))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ``MinimizerResult`` contains all the usual best-fit parameters and\nfitting statistics. For example, the optimal solution from the grid search\nis given below together with a plot:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(fit_report(result_brute))\nplt.plot(x, data, 'b')\nplt.plot(x, data + fcn2min(result_brute.params, x, data), 'r--')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that this fit is already very good, which is what we should expect\nsince our ``brute`` force grid is sampled rather finely and encompasses the\n\"correct\" values.\n\nIn a more realistic, complicated example the ``brute`` method will be used\nto get reasonable values for the parameters and perform another minimization\n(e.g., using ``leastsq``) using those as starting values. That is where the\n`keep`` parameter comes into play: it determines the \"number of best\ncandidates from the brute force method that are stored in the ``candidates``\nattribute\". In the example above we store the best-ranking 25 solutions (the\ndefault value is ``50`` and storing all the grid points can be accomplished\nby choosing ``all``). The ``candidates`` attribute contains the parameters\nand ``chisqr`` from the brute force method as a namedtuple,\n``(\u2018Candidate\u2019, [\u2018params\u2019, \u2018score\u2019])``, sorted on the (lowest) ``chisqr``\nvalue. To access the values for a particular candidate one can use\n``result.candidate[#].params`` or ``result.candidate[#].score``, where a\nlower # represents a better candidate. The ``show_candidates(#)`` uses the\n``pretty_print()`` method to show a specific candidate-# or all candidates\nwhen no number is specified.\n\nThe optimal fit is, as usual, stored in the ``MinimizerResult.params``\nattribute and is, therefore, identical to ``result_brute.show_candidates(1)``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result_brute.show_candidates(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this case, the next-best scoring candidate has already a ``chisqr`` that\nincreased quite a bit:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result_brute.show_candidates(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and is, therefore, probably not so likely... However, as said above, in most\ncases you'll want to do another minimization using the solutions from the\n``brute`` method as starting values. That can be easily accomplished as\nshown in the code below, where we now perform a ``leastsq`` minimization\nstarting from the top-25 solutions and accept the solution if the ``chisqr``\nis lower than the previously 'optimal' solution:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "best_result = copy.deepcopy(result_brute)\n\nfor candidate in result_brute.candidates:\n    trial = fitter.minimize(method='leastsq', params=candidate.params)\n    if trial.chisqr < best_result.chisqr:\n        best_result = trial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the ``leastsq`` minimization we obtain the following parameters for the\nmost optimal result:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(fit_report(best_result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As expected the parameters have not changed significantly as they were\nalready very close to the \"real\" values, which can also be appreciated from\nthe plots below.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.plot(x, data, 'b')\nplt.plot(x, data + fcn2min(result_brute.params, x, data), 'r--',\n         label='brute')\nplt.plot(x, data + fcn2min(best_result.params, x, data), 'g--',\n         label='brute followed by leastsq')\nplt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, the results from the ``brute`` force grid-search can be visualized\nusing the rather lengthy Python function below (which might get incorporated\nin lmfit at some point).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_results_brute(result, best_vals=True, varlabels=None,\n                       output=None):\n    \"\"\"Visualize the result of the brute force grid search.\n\n    The output file will display the chi-square value per parameter and contour\n    plots for all combination of two parameters.\n\n    Inspired by the `corner` package (https://github.com/dfm/corner.py).\n\n    Parameters\n    ----------\n    result : :class:`~lmfit.minimizer.MinimizerResult`\n        Contains the results from the :meth:`brute` method.\n\n    best_vals : bool, optional\n        Whether to show the best values from the grid search (default is True).\n\n    varlabels : list, optional\n        If None (default), use `result.var_names` as axis labels, otherwise\n        use the names specified in `varlabels`.\n\n    output : str, optional\n        Name of the output PDF file (default is 'None')\n    \"\"\"\n    from matplotlib.colors import LogNorm\n\n    npars = len(result.var_names)\n    fig, axes = plt.subplots(npars, npars)\n\n    if not varlabels:\n        varlabels = result.var_names\n    if best_vals and isinstance(best_vals, bool):\n        best_vals = result.params\n\n    for i, par1 in enumerate(result.var_names):\n        for j, par2 in enumerate(result.var_names):\n\n            # parameter vs chi2 in case of only one parameter\n            if npars == 1:\n                axes.plot(result.brute_grid, result.brute_Jout, 'o', ms=3)\n                axes.set_ylabel(r'$\\chi^{2}$')\n                axes.set_xlabel(varlabels[i])\n                if best_vals:\n                    axes.axvline(best_vals[par1].value, ls='dashed', color='r')\n\n            # parameter vs chi2 profile on top\n            elif i == j and j < npars-1:\n                if i == 0:\n                    axes[0, 0].axis('off')\n                ax = axes[i, j+1]\n                red_axis = tuple([a for a in range(npars) if a != i])\n                ax.plot(np.unique(result.brute_grid[i]),\n                        np.minimum.reduce(result.brute_Jout, axis=red_axis),\n                        'o', ms=3)\n                ax.set_ylabel(r'$\\chi^{2}$')\n                ax.yaxis.set_label_position(\"right\")\n                ax.yaxis.set_ticks_position('right')\n                ax.set_xticks([])\n                if best_vals:\n                    ax.axvline(best_vals[par1].value, ls='dashed', color='r')\n\n            # parameter vs chi2 profile on the left\n            elif j == 0 and i > 0:\n                ax = axes[i, j]\n                red_axis = tuple([a for a in range(npars) if a != i])\n                ax.plot(np.minimum.reduce(result.brute_Jout, axis=red_axis),\n                        np.unique(result.brute_grid[i]), 'o', ms=3)\n                ax.invert_xaxis()\n                ax.set_ylabel(varlabels[i])\n                if i != npars-1:\n                    ax.set_xticks([])\n                elif i == npars-1:\n                    ax.set_xlabel(r'$\\chi^{2}$')\n                if best_vals:\n                    ax.axhline(best_vals[par1].value, ls='dashed', color='r')\n\n            # contour plots for all combinations of two parameters\n            elif j > i:\n                ax = axes[j, i+1]\n                red_axis = tuple([a for a in range(npars) if a != i and a != j])\n                X, Y = np.meshgrid(np.unique(result.brute_grid[i]),\n                                   np.unique(result.brute_grid[j]))\n                lvls1 = np.linspace(result.brute_Jout.min(),\n                                    np.median(result.brute_Jout)/2.0, 7, dtype='int')\n                lvls2 = np.linspace(np.median(result.brute_Jout)/2.0,\n                                    np.median(result.brute_Jout), 3, dtype='int')\n                lvls = np.unique(np.concatenate((lvls1, lvls2)))\n                ax.contourf(X.T, Y.T, np.minimum.reduce(result.brute_Jout, axis=red_axis),\n                            lvls, norm=LogNorm())\n                ax.set_yticks([])\n                if best_vals:\n                    ax.axvline(best_vals[par1].value, ls='dashed', color='r')\n                    ax.axhline(best_vals[par2].value, ls='dashed', color='r')\n                    ax.plot(best_vals[par1].value, best_vals[par2].value, 'rs', ms=3)\n                if j != npars-1:\n                    ax.set_xticks([])\n                elif j == npars-1:\n                    ax.set_xlabel(varlabels[i])\n                if j - i >= 2:\n                    axes[i, j].axis('off')\n\n    if output is not None:\n        plt.savefig(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and finally, to generated the figure:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_results_brute(result_brute, best_vals=True, varlabels=None)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
